{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2756a27b-6626-4396-9bc7-fb490a8fdef7",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d90539-1e9e-47a2-9747-a86b23d4e87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "import torch, json, re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03f0b6a-9d05-43d4-b7d4-09ccf8c66a43",
   "metadata": {},
   "source": [
    "# Compute predictions from validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbb5733-a67d-4508-b8b5-d222cdc6ba6f",
   "metadata": {},
   "source": [
    "## Predictions for model trained on final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dd8db2-1e9e-40a7-a9cd-e77be6f102a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the validation set file\n",
    "json_path = \"validation_data.json\"\n",
    "\n",
    "# Extract commit messages and grades\n",
    "commit_messages = extract_commit_messages(json_path)\n",
    "validation_grades = extract_grades(json_path)\n",
    "\n",
    "# Validate the first num_messages_to_validate messages\n",
    "num_messages_to_validate = 5 # len(commit_messages)\n",
    "\n",
    "# Load the English language spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# This is the path to BERT large uncased trained on our final dataset\n",
    "model_path = \"./bert_grade_model\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Run validation (set fake to False if you want to use the API key to determine whether the body of a message is meaningful\n",
    "custom_accuracy, custom_f1, predicted_grades, actual_grades = actual_vs_pred(\n",
    "    commit_messages, validation_grades, num_messages_to_validate, nlp, tokenizer, model, no_openai=True)\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\nF1 Score: {custom_f1:.2f}\")\n",
    "print(f\"Accuracy: {custom_accuracy:.2f}\")\n",
    "\n",
    "# Save predictions to txt\n",
    "np.savetxt('y_final_pred.txt', predicted_grades, delimiter=' ', fmt='%.2f')\n",
    "np.savetxt('y_true.txt', actual_grades, delimiter=' ', fmt='%.2f')\n",
    "print(\"Saved predicted grades and actual grades to y_final_pred.txt and y_true.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c7c185-e1e2-4df0-a5bd-b9f00ab1598f",
   "metadata": {},
   "source": [
    "## Predictions for model trained on old dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545ad439-2dea-4174-914d-1c83433ad4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is specific to this cell in this notebook and is therefore not added into the readme\n",
    "def predict_grade(commit_message, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Grades commit messages using provided model and tokenizer\n",
    "\n",
    "    Args:\n",
    "        commit_message (str): Commit message\n",
    "        model: Loaded model to be used\n",
    "        tokenizer: Loaded model to be used\n",
    "\n",
    "    Returns:\n",
    "        predicted_label: Predicted grade (int)\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = tokenizer(commit_message, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    inputs = {key: value.to(\"cpu\") for key, value in inputs.items()}  # Move inputs to the model's device\n",
    "    outputs = model(**inputs)\n",
    "    probabilities = outputs.logits.softmax(dim=1)\n",
    "    predicted_label = probabilities.argmax().item()\n",
    "    return predicted_label\n",
    "\n",
    "\n",
    "\n",
    "# Number of commits to evaluate (n = 0 if whole file must be evaluated)\n",
    "n = 5 # 0\n",
    "\n",
    "# This is the path to BERT large uncased trained on our original dataset\n",
    "model_path=\"./bert_old_model\"\n",
    "\n",
    "# Loader tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "print(\"Loaded model\")\n",
    "\n",
    "# Open json file\n",
    "with open('validation_data.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Count correct answers + number of answers differing by 1, 2 and 3 from actual grade\n",
    "correct = 0\n",
    "err1 = 0\n",
    "err2 = 0\n",
    "err3 = 0\n",
    "err4 = 0\n",
    "err5 = 0\n",
    "graded = 0\n",
    "\n",
    "# Initialize empty arrays\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "print(\"Started grading\")\n",
    "\n",
    "for i, cm in enumerate(data):\n",
    "    c = cm[\"commit_message\"]\n",
    "    real_gr = cm[\"grade\"]\n",
    "    gr = predict_grade(c, model, tokenizer)\n",
    "    y_pred.append(gr)\n",
    "    y_true.append(real_gr)\n",
    "\n",
    "    graded += 1\n",
    "    if graded >= n and n > 0:\n",
    "        break\n",
    "    \n",
    "    if gr != real_gr:\n",
    "        diff = np.abs(gr - real_gr)\n",
    "        if diff == 1:\n",
    "            err1 += 1\n",
    "        elif diff == 2:\n",
    "            err2 +=1\n",
    "        elif diff == 3:\n",
    "            err3 +=1\n",
    "        elif diff == 4:\n",
    "            err4 += 1\n",
    "        elif diff == 5:\n",
    "            err5 += 1\n",
    "    else:\n",
    "        correct += 1\n",
    "\n",
    "print(f\"Model correctly guessed {correct}/{graded} | err1 = {err1}, err2 = {err2}, err3 = {err3}, err4 = {err4}, err5 = {err5}\")\n",
    "\n",
    "np.savetxt('y_old_pred.txt', y_pred, delimiter=' ', fmt='%.2f')\n",
    "print(\"Saved predicted grades to y_old_pred.txt\")\n",
    "# No need to save y_true again since it should be the same for both sets of predictions\n",
    "# np.savetxt('y_true.txt', y_true, delimiter=' ', fmt='%.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d629d18-2f08-450c-95a1-56d0ee5cc13d",
   "metadata": {},
   "source": [
    "# Compute metrics using previously generated predictions\n",
    "**Important:** this cell will only work if at least one instance of each classe (0-5) is present in `y_final_pred.txt` and `y_old_pred.txt`, so it's best to run validation on the entire validation dataset (`validation_data.json`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64113ddc-3287-496a-9236-c869b148a9b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# How to load preds/real grades from txt if necessary\n",
    "y_pred = np.loadtxt('y_final_pred.txt', delimiter=' ', dtype=float)\n",
    "y_old_pred = np.loadtxt('y_old_pred.txt', delimiter=' ', dtype=float)\n",
    "y_true = np.loadtxt('y_true.txt', delimiter=' ', dtype=float)\n",
    "\n",
    "y_true = y_true.astype(int)\n",
    "y_old_pred = y_old_pred.astype(int)\n",
    "\n",
    "# Map y_pred to integers (with a margin of error of 0.5, e.g. if \n",
    "# Compute the fractional parts\n",
    "fractional_part = y_pred - np.floor(y_pred)\n",
    "\n",
    "# Mask for elements that end in .5\n",
    "mask_half = (fractional_part == 0.5)\n",
    "\n",
    "# Compute floor and ceil for all values\n",
    "low = np.floor(y_pred)\n",
    "high = np.ceil(y_pred)\n",
    "\n",
    "# Compute differences to see which integer is closer to y_true\n",
    "diff_low = np.abs(y_true - low)\n",
    "diff_high = np.abs(y_true - high)\n",
    "\n",
    "# Determine which side is closer where we have .5\n",
    "closer_low = diff_low <= diff_high\n",
    "\n",
    "# Start with a copy of y_pred\n",
    "y_pred_final = y_pred.copy()\n",
    "\n",
    "# For .5 values, pick either low or high based on closeness to y_true\n",
    "y_pred_final[mask_half & closer_low] = low[mask_half & closer_low]\n",
    "y_pred_final[mask_half & ~closer_low] = high[mask_half & ~closer_low]\n",
    "\n",
    "labels_str = ['0', '1', '2', '3', '4', '5']\n",
    "labels = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "# Cast to ensure that all values are ints\n",
    "y_pred_final = y_pred_final.astype(int)\n",
    "\n",
    "cm_final = confusion_matrix(y_true, y_pred_final, labels=labels)\n",
    "\n",
    "# Visualize the confusion matrix and report for final DS\n",
    "disp_final = ConfusionMatrixDisplay(confusion_matrix=cm_final, display_labels=labels)\n",
    "disp_final.plot(cmap=plt.cm.Blues)\n",
    "plt.title(f\"Confusion Matrix for {len(labels)}-Class Problem (final DS)\")\n",
    "# plt.savefig(\"cm_final_ds.png\", dpi=200)\n",
    "plt.show()\n",
    "\n",
    "report_final = classification_report(y_true, y_pred_final, target_names=labels_str, zero_division=0)\n",
    "print(f\"Report for final DS:\\n{report_final}\")\n",
    "\n",
    "print(\"\\n\"*3)\n",
    "\n",
    "# For old dataset, there exist no 0s in true or predicted values\n",
    "labels_old = [1, 2, 3, 4, 5]\n",
    "labels_old_str = ['1', '2', '3', '4','5']\n",
    "# Visualize the confusion matrix for old DS\n",
    "cm_old = confusion_matrix(y_true, y_old_pred, labels=labels_old)\n",
    "\n",
    "disp_old = ConfusionMatrixDisplay(confusion_matrix=cm_old, display_labels=labels_old)\n",
    "disp_old.plot(cmap=plt.cm.Blues)\n",
    "plt.title(f\"Confusion Matrix for {len(labels)}-Class Problem (old DS)\")\n",
    "# plt.savefig(\"cm_old_ds.png\", dpi=200)\n",
    "plt.show()\n",
    "\n",
    "report_old = classification_report(y_true, y_old_pred, target_names=labels_old_str, zero_division=0)\n",
    "print(f\"Report for old DS:\\n{report_old}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
